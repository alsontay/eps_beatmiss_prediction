{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22160,"status":"ok","timestamp":1668045498701,"user":{"displayName":"Anna Ying (Arcanum)","userId":"09943394831364595451"},"user_tz":-480},"id":"HdVESKHzQThE","outputId":"354d3d65-4321-43f4-944c-2d8458221a2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/kevctae/twint.git\n","  Cloning https://github.com/kevctae/twint.git to /tmp/pip-req-build-y2sz0sbi\n","  Running command git clone -q https://github.com/kevctae/twint.git /tmp/pip-req-build-y2sz0sbi\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from twint==2.1.21) (3.8.3)\n","Collecting aiodns\n","  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from twint==2.1.21) (4.6.3)\n","Collecting cchardet\n","  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n","\u001b[K     |████████████████████████████████| 263 kB 4.2 MB/s \n","\u001b[?25hCollecting dataclasses\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Collecting elasticsearch\n","  Downloading elasticsearch-8.5.0-py3-none-any.whl (385 kB)\n","\u001b[K     |████████████████████████████████| 385 kB 42.3 MB/s \n","\u001b[?25hRequirement already satisfied: pysocks in /usr/local/lib/python3.7/dist-packages (from twint==2.1.21) (1.7.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from twint==2.1.21) (1.3.5)\n","Collecting aiohttp_socks\n","  Downloading aiohttp_socks-0.7.1-py3-none-any.whl (9.3 kB)\n","Collecting schedule\n","  Downloading schedule-1.1.0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: geopy in /usr/local/lib/python3.7/dist-packages (from twint==2.1.21) (1.17.0)\n","Collecting fake-useragent\n","  Downloading fake_useragent-0.1.14-py3-none-any.whl (13 kB)\n","Collecting googletransx\n","  Downloading googletransx-2.4.2.tar.gz (13 kB)\n","Collecting pycares>=4.0.0\n","  Downloading pycares-4.2.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n","\u001b[K     |████████████████████████████████| 288 kB 36.6 MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pycares>=4.0.0->aiodns->twint==2.1.21) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->twint==2.1.21) (2.21)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint==2.1.21) (6.0.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint==2.1.21) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint==2.1.21) (1.3.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint==2.1.21) (0.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint==2.1.21) (4.1.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint==2.1.21) (1.8.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint==2.1.21) (22.1.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint==2.1.21) (1.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint==2.1.21) (2.1.1)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->twint==2.1.21) (2.10)\n","Collecting python-socks[asyncio]<3.0.0,>=2.0.0\n","  Downloading python_socks-2.0.3-py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 6.0 MB/s \n","\u001b[?25hCollecting elastic-transport<9,>=8\n","  Downloading elastic_transport-8.4.0-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 6.2 MB/s \n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elastic-transport<9,>=8->elasticsearch->twint==2.1.21) (2022.9.24)\n","Collecting urllib3<2,>=1.26.2\n","  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 42.4 MB/s \n","\u001b[?25hRequirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.7/dist-packages (from geopy->twint==2.1.21) (1.52)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from googletransx->twint==2.1.21) (2.23.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->twint==2.1.21) (2022.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->twint==2.1.21) (2.8.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->twint==2.1.21) (1.21.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->twint==2.1.21) (1.15.0)\n","Collecting requests\n","  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n","\u001b[?25hBuilding wheels for collected packages: twint, googletransx\n","  Building wheel for twint (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for twint: filename=twint-2.1.21-py3-none-any.whl size=39180 sha256=7141771736bc91ea7fb92fc4cdf737430a1a4512a1896d083c8517d60b6b13a5\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-nj9kcwaw/wheels/37/b7/73/3f444bfba5ddc96a71f205dd9a8361f1336e2b8376ea8fefed\n","  Building wheel for googletransx (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletransx: filename=googletransx-2.4.2-py3-none-any.whl size=15968 sha256=c87f65a189a09df4aa1e579de0c0369d6837258d355d2f09eea2b727988e86ff\n","  Stored in directory: /root/.cache/pip/wheels/66/d5/b1/31104b338f7fd45aa8f7d22587765db06773b13df48a89735f\n","Successfully built twint googletransx\n","Installing collected packages: urllib3, python-socks, requests, pycares, elastic-transport, schedule, googletransx, fake-useragent, elasticsearch, dataclasses, cchardet, aiohttp-socks, aiodns, twint\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","Successfully installed aiodns-3.0.0 aiohttp-socks-0.7.1 cchardet-2.1.7 dataclasses-0.6 elastic-transport-8.4.0 elasticsearch-8.5.0 fake-useragent-0.1.14 googletransx-2.4.2 pycares-4.2.2 python-socks-2.0.3 requests-2.28.1 schedule-1.1.0 twint-2.1.21 urllib3-1.26.12\n"]}],"source":["!pip install --upgrade git+https://github.com/kevctae/twint.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5055,"status":"ok","timestamp":1668045523617,"user":{"displayName":"Anna Ying (Arcanum)","userId":"09943394831364595451"},"user_tz":-480},"id":"ZAokSO70QaAI","outputId":"9cd22253-fbc1-418d-e5f8-86e06f8245b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting nest_asyncio\n","  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n","Installing collected packages: nest-asyncio\n","Successfully installed nest-asyncio-1.5.6\n"]}],"source":["!pip install nest_asyncio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IFlGmUNIQThR"},"outputs":[],"source":["# Import required packages\n","\n","import pandas as pd\n","import numpy as np\n","import twint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SGVZjFtQThU"},"outputs":[],"source":["# Import and apply required package to solve compatibility issues with Python notebooks and runtime errors\n","\n","import nest_asyncio\n","nest_asyncio.apply()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"picRUPVeQThW"},"outputs":[],"source":["# Read Yahoo Finance csv file\n","\n","collated_df = pd.read_csv('..\\data\\meta\\Yahoo Finance.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MqT8wq5zQThY"},"outputs":[],"source":["# Convert earnings dates and previous earnings dates to datetime format\n","extraction_df = collated_df[['Ticker', 'Earnings Date', 'Previous Earnings Date']].copy(deep = True)\n","extraction_df['Earnings Date'] = pd.to_datetime(extraction_df['Earnings Date'], format = '%d/%m/%Y')\n","extraction_df['Previous Earnings Date'] = pd.to_datetime(extraction_df['Previous Earnings Date'], format = '%d/%m/%Y')\n","\n","# Convert earnings dates and previous earnings dates to string format that can be parsed into Twint\n","extraction_df['Earnings Date'] = extraction_df['Earnings Date'].dt.strftime('%Y-%m-%d')\n","extraction_df['Previous Earnings Date'] = extraction_df['Previous Earnings Date'].dt.strftime('%Y-%m-%d')\n","\n","# Convert required values to list format for more efficient iteration over rows\n","extraction_lst = extraction_df.values.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"viX5RtHIQThq","outputId":"f06a83ed-d46d-4230-e394-1144f4e50915"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initiating TA 2022-08-01 search...\n","Running TA 2022-08-01 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2022-08-01 search.\n","Initiating TA 2022-05-02 search...\n","Running TA 2022-05-02 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2022-05-02 search.\n","Initiating TA 2022-02-23 search...\n","Running TA 2022-02-23 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2022-02-23 search.\n","Initiating TA 2021-11-01 search...\n","Running TA 2021-11-01 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2021-11-01 search.\n","Initiating TA 2021-08-02 search...\n","Running TA 2021-08-02 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2021-08-02 search.\n","Initiating TA 2021-05-03 search...\n","Running TA 2021-05-03 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2021-05-03 search.\n","Initiating TA 2021-02-25 search...\n","Running TA 2021-02-25 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2021-02-25 search.\n","Initiating TA 2020-11-04 search...\n","Running TA 2020-11-04 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2020-11-04 search.\n","Initiating TA 2020-08-05 search...\n","Running TA 2020-08-05 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2020-08-05 search.\n","Initiating TA 2020-05-05 search...\n","Running TA 2020-05-05 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2020-05-05 search.\n","Initiating TA 2020-02-25 search...\n","Running TA 2020-02-25 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2020-02-25 search.\n","Initiating TA 2019-11-05 search...\n","Running TA 2019-11-05 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2019-11-05 search.\n","Initiating COIN 2022-08-09 search...\n","Running COIN 2022-08-09 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed COIN 2022-08-09 search.\n","Initiating COIN 2022-05-10 search...\n","Running COIN 2022-05-10 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed COIN 2022-05-10 search.\n","Initiating COIN 2022-02-24 search...\n","Running COIN 2022-02-24 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed COIN 2022-02-24 search.\n","Initiating COIN 2021-11-09 search...\n","Running COIN 2021-11-09 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed COIN 2021-11-09 search.\n","Initiating COIN 2021-08-10 search...\n","Running COIN 2021-08-10 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed COIN 2021-08-10 search.\n"]}],"source":["# Run Twint search for top 58 stocks per quarter and output the results to CSV\n","# Search for Twitter mentions containing the \"$\" symbol, e.g. \"$TSLA\"\n","# Filter out all retweets, which will be considered later\n","# Add a filter for Tweets with at least 2 likes because of time and resource constraints\n","\n","for ticker in extraction_lst[:588]:\n","    print(\"Initiating \" + ticker[0] + \" \" + ticker[1] + \" search...\")\n","    config = twint.Config()\n","    config.Search = \"$\" + ticker[0]\n","    config.Lang = \"en\"\n","    config.Since = ticker[2]\n","    config.Until = ticker[1]\n","    config.Hide_output = True\n","    config.Min_likes = 2\n","    config.Filter_retweets = True\n","    config.Pandas = True\n","    # Please uncomment this line if you wish to save CSV files\n","    # config.Store_csv = True\n","    config.Output = f'../data/datafiles/twitter/{ticker[0]}_Tweets_{ticker[1]}.csv'\n","\n","    print(\"Running \" + ticker[0] + \" \" + ticker[1] + \" search...\")\n","    twint.run.Search(config)\n","\n","    print(\"Completed \" + ticker[0] + \" \" + ticker[1] + \" search.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SbcVSBf2TyvV","outputId":"f06a83ed-d46d-4230-e394-1144f4e50915"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initiating TA 2022-08-01 search...\n","Running TA 2022-08-01 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2022-08-01 search.\n","Initiating TA 2022-05-02 search...\n","Running TA 2022-05-02 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2022-05-02 search.\n","Initiating TA 2022-02-23 search...\n","Running TA 2022-02-23 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2022-02-23 search.\n","Initiating TA 2021-11-01 search...\n","Running TA 2021-11-01 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2021-11-01 search.\n","Initiating TA 2021-08-02 search...\n","Running TA 2021-08-02 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2021-08-02 search.\n","Initiating TA 2021-05-03 search...\n","Running TA 2021-05-03 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2021-05-03 search.\n","Initiating TA 2021-02-25 search...\n","Running TA 2021-02-25 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2021-02-25 search.\n","Initiating TA 2020-11-04 search...\n","Running TA 2020-11-04 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2020-11-04 search.\n","Initiating TA 2020-08-05 search...\n","Running TA 2020-08-05 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2020-08-05 search.\n","Initiating TA 2020-05-05 search...\n","Running TA 2020-05-05 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2020-05-05 search.\n","Initiating TA 2020-02-25 search...\n","Running TA 2020-02-25 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2020-02-25 search.\n","Initiating TA 2019-11-05 search...\n","Running TA 2019-11-05 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed TA 2019-11-05 search.\n","Initiating COIN 2022-08-09 search...\n","Running COIN 2022-08-09 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed COIN 2022-08-09 search.\n","Initiating COIN 2022-05-10 search...\n","Running COIN 2022-05-10 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed COIN 2022-05-10 search.\n","Initiating COIN 2022-02-24 search...\n","Running COIN 2022-02-24 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed COIN 2022-02-24 search.\n","Initiating COIN 2021-11-09 search...\n","Running COIN 2021-11-09 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed COIN 2021-11-09 search.\n","Initiating COIN 2021-08-10 search...\n","Running COIN 2021-08-10 search...\n","[!] No more data! Scraping will stop now.\n","found 0 deleted tweets in this search.\n","Completed COIN 2021-08-10 search.\n"]}],"source":["# Run Twint search for the remaining 40 stocks per quarter and output the results to CSV\n","# Search for Twitter mentions containing the \"$\" symbol, e.g. \"$TSLA\"\n","# Filter out all retweets, which will be considered later\n","\n","for ticker in extraction_lst[588:]:\n","    print(\"Initiating \" + ticker[0] + \" \" + ticker[1] + \" search...\")\n","    config = twint.Config()\n","    config.Search = \"$\" + ticker[0]\n","    config.Lang = \"en\"\n","    config.Since = ticker[2]\n","    config.Until = ticker[1]\n","    config.Hide_output = True\n","    # config.Min_likes = 2\n","    config.Filter_retweets = True\n","    config.Pandas = True\n","    # Please uncomment this line if you wish to save CSV files\n","    # config.Store_csv = True\n","    config.Output = f'../data/datafiles/twitter/{ticker[0]}_Tweets_{ticker[1]}.csv'\n","\n","    print(\"Running \" + ticker[0] + \" \" + ticker[1] + \" search...\")\n","    twint.run.Search(config)\n","\n","    print(\"Completed \" + ticker[0] + \" \" + ticker[1] + \" search.\")"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.7.6 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"5b020d37e2d2792244d38dc749b48635155a26318d10fe99dbb63be9f4ff243c"}}},"nbformat":4,"nbformat_minor":0}
