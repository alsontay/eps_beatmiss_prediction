{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3080,"status":"ok","timestamp":1668078798815,"user":{"displayName":"Anna Ying (Arcanum)","userId":"09943394831364595451"},"user_tz":-480},"id":"5aZk6YhyPhuc","outputId":"a7deb296-ceb5-4118-f92b-ee209eeecae0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting rbo\n","  Downloading rbo-0.1.2-py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: numpy<2.0,>=1.18 in /usr/local/lib/python3.7/dist-packages (from rbo) (1.21.6)\n","Installing collected packages: rbo\n","Successfully installed rbo-0.1.2\n"]}],"source":["# Install package for Rank-Biased Overlap\n","!pip install rbo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CYPGr-SNAvA"},"outputs":[],"source":["# Import required packages\n","import pandas as pd\n","import numpy as np\n","import os\n","import rbo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"veyeCw2JPI5_"},"outputs":[],"source":["# Read Reddit ranked list and convert to list for RBO analysis\n","reddit_df = pd.read_csv('../data/meta/toptrending_100_stocks.txt')\n","reddit_lst = reddit_df['Ticker'].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6YDxRjSmPKPl"},"outputs":[],"source":["# Read Yahoo Finance CSV file\n","collated_df = pd.read_csv('../data/meta/Yahoo Finance.csv')\n","# Convert Earnings Date and Previous Earnings Date to datetime format\n","collated_df['Earnings Date'] = pd.to_datetime(collated_df['Earnings Date'], format = '%d/%m/%Y')\n","collated_df['Earnings Date'] = collated_df['Earnings Date'].dt.strftime('%Y-%m-%d')\n","# Extract the required columns and convert to list for more efficient iteration over rows\n","sentiment_df = collated_df[['Ticker', 'Earnings Date']].copy(deep = True)\n","# Filter only for earnings dates in 2022 to match Reddit rankings\n","sentiment_df = sentiment_df.loc[sentiment_df['Earnings Date'] > '2021-12-31']\n","sentiment_lst = sentiment_df.values.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HySShT8X71wj"},"outputs":[],"source":["# Change Directory for Twitter Data\n","os.chdir('../data/datafiles/twitter/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9XKsDNfPLuM"},"outputs":[],"source":["# Initialize a list to collect files not found\n","fnf = []\n","# Initialize a dictionary to collect Twitter mentions found\n","twitter_dict = {}\n","\n","# Loop through each stock per quarter in the Yahoo Finance list\n","for idx, ticker in enumerate(sentiment_lst):\n","    # Format the file name\n","    file = f'{ticker[0]}_Tweets_{ticker[1]}.csv'\n","    print(f'Initiating {idx}, {ticker[0]} {ticker[1]} collation...')\n","\n","    try:\n","        # Read the CSV file if it exists\n","        ticker_df = pd.read_csv(file)\n","        # Convert date column to datetime format\n","        ticker_df['date'] = pd.to_datetime(ticker_df['date'], format = '%Y-%m-%d')\n","        # Filter only for Tweets posted in 2022 to match Reddit rankings\n","        mask = ticker_df['date'] > '2021-12-31'\n","        twitter_dict[f'{ticker[0]}'] = twitter_dict.get(f'{ticker[0]}', 0) + len(ticker_df.loc[mask])\n","        \n","        print(f'Completed {idx}, {ticker[0]} {ticker[1]} collation.')\n","    \n","    # If file not found, append to fnf list\n","    except FileNotFoundError:\n","        print(f'File Not Found: {idx}, {file}')\n","        fnf.append((idx, ticker))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIAtvqGB_C1I"},"outputs":[],"source":["# Check for files not found - Should have only 1 observation\n","print(len(fnf), fnf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Va9LGH3PQhL"},"outputs":[],"source":["# Convert Twitter mentions dictionary to list for RBO analysis\n","twitter_lst = []\n","twitter_keys = sorted(twitter_dict, key=twitter_dict.get, reverse=True)\n","for k in twitter_keys:\n","    twitter_lst.append(k)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2sFJxz4CPUFG"},"outputs":[],"source":["# Calculate RBO for ranking similarity score\n","\n","rbo.RankingSimilarity(reddit_lst, twitter_lst).rbo()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zpj21Ga9PZGG"},"outputs":[],"source":["# Print top 10 rankings for both Reddit and Twitter\n","\n","print(\"\\033[4mReddit\\033[0m\", \"\\033[4mTwitter\\033[0m\")\n","for i in range(10):\n","    print(f'{reddit_lst[i]}\\t{twitter_lst[i]}')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM63pMCBdlmJG/g/b429JLe","collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.7.6 ('base')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.6"},"vscode":{"interpreter":{"hash":"5b020d37e2d2792244d38dc749b48635155a26318d10fe99dbb63be9f4ff243c"}}},"nbformat":4,"nbformat_minor":0}
